The primary objective of this project is to develop a comprehensive and robust Sign Language Recognition System using neural net models like Convolutional Neural Network (CNN) and advanced preprocessing techniques. This will leverage technology to improve communication methods for the DHH community of India. 
The system is a vision based approach. All the signs are represented with bare hands and so it eliminates the problem of using any artificial devices for interaction.

For the project we tried to find already made datasets but we couldn’t find datasets in the form of raw images that matched our requirements. All we could find were the datasets in the form of RGB values. Hence we decided to create our own data set. Steps we followed to create our data set are as follows.
We used the Open computer vision(OpenCV) library in order to produce our dataset.Firstly we captured around 800 images of each of the symbols in ASL for training purposes and around 200 images per symbol for testing purposes. First we capture each frame shown by the webcam of our machine. In each frame we define a region of interest (ROI) which is denoted by a blue bounded square as shown in the image below.

After converting the image to grayscale, the function applies a Gaussian blur filter using ‘GaussianBlur’, which helps to reduce noise and smooth the image, which is particularly important for improving the results of the thresholding operations that follow. The kernel size for the blur is set to (5, 5), and the standard deviation is set to 2, which controls the extent of the blurring effect. We apply our gaussian blur filter to our image as it helps us extract various features of our image. 
We begin by preparing the image data from specified directories (data/train and data/test), applying extensive data augmentation techniques like resizing, rotation, zooming, and shifting to enhance the training set's diversity and improve model generalization. It then constructs two separate convolutional neural networks (CNNs) based on the MobileNetV2 architecture: a "primary model" and a "verification model," both pre-trained on ImageNet to leverage transfer learning. The training proceeds in two phases for the primary model: an initial phase where only the newly added classification layers are trained while the MobileNetV2 base is frozen, followed by a fine-tuning phase where a portion of the base model's upper layers are unfrozen and trained with a very low learning rate. The verification model is trained separately. After training, the script saves both models' architectures and weights for later deployment. Finally, it evaluates the primary model's performance on the test set, implementing a sophisticated post-prediction filtering mechanism that uses the verification model and a K-Nearest Neighbors (KNN) algorithm; this system re-checks low-confidence predictions and cases involving easily confused sign pairs (like 'M' and 'N'), aiming to improve accuracy for challenging classifications. The entire process is visualized through plots of training history and a confusion matrix, culminating in a printout of key evaluation metrics such as accuracy, precision, recall, and F1-score.
We define a Streamlit application named SignRecognitionApp designed for real-time sign language recognition using a webcam. Upon initialization, the app attempts to load two pre-trained deep learning models (a primary and a verification model) from a specified 'model/' directory, which are expected to be optimized for sign classification, along with a pre-configured StandardScaler and NearestNeighbors for feature processing and KNN-based re-verification. The core run method sets up the Streamlit UI, including a dynamically updated background featuring a faded sign language-related image and a custom-styled "Start Webcam" checkbox that functions as an attractive toggle button. When the webcam is active, it continuously captures video frames, crops a region of interest (ROI) representing the hand sign, preprocesses this ROI, and feeds it to the loaded models for real-time prediction. A sophisticated verification logic is applied: if the primary model's confidence is low, a secondary model is consulted, and if necessary, a K-Nearest Neighbors (KNN) algorithm is used for further re-checking, particularly for known problematic sign pairs, to enhance prediction accuracy. Finally, a temporal smoothing mechanism averages recent predictions to provide a more stable and reliable recognized sign displayed on the Streamlit interface.
